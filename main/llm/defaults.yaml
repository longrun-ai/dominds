providers:
  codex:
    name: Codex (ChatGPT)
    apiType: codex
    baseUrl: https://chatgpt.com/backend-api/
    apiKeyEnvVar: CODEX_HOME
    tech_spec_url: https://platform.openai.com/docs/api-reference/responses
    api_mgmt_url: https://chatgpt.com/
    # `model_param_options` documents common `.minds/team.yaml` `model_params` fields
    # that this provider supports (as of this defaults.yaml).
    #
    # These are documentation hints for humans/agents; they are not auto-applied.
    model_param_options:
      general:
        max_tokens:
          # Provider-agnostic cap on generated tokens.
          type: integer
          min: 1
          description: Maximum tokens to generate (provider-agnostic override).
      codex:
        reasoning_effort:
          prominent: true
          default: high
          # Trade latency/cost for quality on reasoning-capable models.
          # Allowed: none | minimal | low | medium | high | xhigh
          type: enum
          values: [none, minimal, low, medium, high, xhigh]
          description: Reasoning effort level (when supported by the model).
        verbosity:
          prominent: true
          default: medium
          # Control response detail level on GPT-5 series models.
          # Allowed: low | medium | high
          type: enum
          values: [low, medium, high]
          description: Response verbosity/detail level (GPT-5 series).
        temperature:
          # Randomness; use 0–0.2 for tool-calling / deterministic behavior.
          type: number
          min: 0
          max: 2
          description: Sampling temperature (0–2).
        top_p:
          # Nucleus sampling; usually leave unset if using temperature.
          type: number
          min: 0
          max: 1
          description: Nucleus sampling probability (0–1).
        parallel_tool_calls:
          type: boolean
          description: Allow the model to emit parallel tool calls (enable by default in Dominds).
        web_search:
          prominent: true
          default: live
          type: enum
          values: [disabled, cached, live]
          description: Enable native web search tool mode (disabled/cached/live).
        json_response:
          type: boolean
          description: Force JSON object output mode.
        max_tokens:
          type: integer
          min: 1
          description: Provider-specific max tokens override.
    models:
      gpt-5.3-codex:
        name: GPT-5.3 Codex
        optimal_max_tokens: 200000
        # Caution remediation reinjection cadence in generation turns (default: 10).
        caution_remediation_cadence_generations: 10
        context_length: 272000
        input_length: 272000
        output_length: 32768
        context_window: '272K'
      gpt-5.3-codex-spark:
        name: GPT-5.3 Codex Spark
        optimal_max_tokens: 80000
        # Caution remediation reinjection cadence in generation turns (default: 10).
        caution_remediation_cadence_generations: 3
        context_length: 128000
        input_length: 128000
        output_length: 32768
        context_window: '128K'
      gpt-5.2-codex:
        name: GPT-5.2 Codex
        optimal_max_tokens: 200000
        # Caution remediation reinjection cadence in generation turns (default: 10).
        caution_remediation_cadence_generations: 10
        context_length: 272000
        input_length: 272000
        output_length: 32768
        context_window: '272K'
      gpt-5.2:
        name: GPT-5.2
        optimal_max_tokens: 200000
        # Caution remediation reinjection cadence in generation turns (default: 10).
        caution_remediation_cadence_generations: 10
        context_length: 272000
        input_length: 272000
        output_length: 32768
        context_window: '272K'
      gpt-5.1-codex-mini:
        name: GPT-5.1 Codex Mini
        optimal_max_tokens: 200000
        context_length: 272000
        input_length: 272000
        output_length: 32768
        context_window: '272K'
      gpt-5.1-codex-max:
        name: GPT-5.1 Codex Max
        optimal_max_tokens: 200000
        context_length: 272000
        input_length: 272000
        output_length: 32768
        context_window: '272K'
  minimaxi.com-coding-plan:
    name: MiniMax CN Coding Plan
    apiType: anthropic
    baseUrl: https://api.minimaxi.com/anthropic
    apiKeyEnvVar: MINIMAX_CN_CP_API_KEY
    tech_spec_url: https://platform.minimaxi.com/docs/api-reference/text-anthropic-api
    api_mgmt_url: https://platform.minimaxi.com/
    model_param_options:
      general:
        max_tokens:
          type: integer
          min: 1
          description: Fully supported. Maximum number of tokens to generate.
      anthropic:
        temperature:
          type: number
          min: 0
          max: 1
          default: 1
          description: 'Fully supported. Range (0.0, 1.0], controls output randomness; recommended value: 1.'
        top_p:
          type: number
          min: 0
          max: 1
          description: Fully supported. Nucleus sampling parameter.
        json_response:
          type: boolean
          description: Force JSON object output via Anthropic tool_choice mode.
    models:
      MiniMax-M2.5:
        name: MiniMax M2.5
        context_length: 204800
        input_length: 204800
        output_length: 8192
        context_window: '204K'
      MiniMax-M2.1:
        name: MiniMax M2.1
        context_length: 204800
        input_length: 204800
        output_length: 8192
        context_window: '204K'
      MiniMax-M2:
        name: MiniMax M2
        context_length: 204800
        input_length: 204800
        output_length: 8192
        context_window: '204K'
  minimaxi.com:
    name: MiniMax CN
    apiType: anthropic
    baseUrl: https://api.minimaxi.com/anthropic
    apiKeyEnvVar: MINIMAX_CN_API_KEY
    tech_spec_url: https://platform.minimaxi.com/docs/api-reference/text-anthropic-api
    api_mgmt_url: https://platform.minimaxi.com/
    model_param_options:
      general:
        max_tokens:
          type: integer
          min: 1
          description: Fully supported. Maximum number of tokens to generate.
      anthropic:
        temperature:
          type: number
          min: 0
          max: 1
          default: 1
          description: 'Fully supported. Range (0.0, 1.0], controls output randomness; recommended value: 1.'
        top_p:
          type: number
          min: 0
          max: 1
          description: Fully supported. Nucleus sampling parameter.
        json_response:
          type: boolean
          description: Force JSON object output via Anthropic tool_choice mode.
    models:
      MiniMax-M2.5:
        name: MiniMax M2.5
        context_length: 204800
        input_length: 204800
        output_length: 8192
        context_window: '204K'
      MiniMax-M2.1:
        name: MiniMax M2.1
        context_length: 204800
        input_length: 204800
        output_length: 8192
        context_window: '204K'
      MiniMax-M2:
        name: MiniMax M2 Stable
        context_length: 204800
        input_length: 204800
        output_length: 8192
        context_window: '204K'
  minimax.io-coding-plan:
    name: MiniMax International Coding Plan
    apiType: anthropic
    baseUrl: https://api.minimax.io/anthropic
    apiKeyEnvVar: MINIMAX_CP_API_KEY
    tech_spec_url: https://platform.minimax.io/docs/api-reference/text-anthropic-api
    api_mgmt_url: https://platform.minimax.io/
    model_param_options:
      general:
        max_tokens:
          type: integer
          min: 1
          description: Fully supported. Maximum number of tokens to generate.
      anthropic:
        temperature:
          type: number
          min: 0
          max: 1
          default: 1
          description: 'Fully supported. Range (0.0, 1.0], controls output randomness; recommended value: 1.'
        top_p:
          type: number
          min: 0
          max: 1
          description: Fully supported. Nucleus sampling parameter.
        json_response:
          type: boolean
          description: Force JSON object output via Anthropic tool_choice mode.
    models:
      MiniMax-M2.5:
        name: MiniMax M2.5
        context_length: 204800
        input_length: 204800
        output_length: 8192
        context_window: '204K'
      MiniMax-M2.1:
        name: MiniMax M2.1
        context_length: 204800
        input_length: 204800
        output_length: 8192
        context_window: '204K'
      MiniMax-M2:
        name: MiniMax M2
        context_length: 204800
        input_length: 204800
        output_length: 8192
        context_window: '204K'
  minimax.io:
    name: MiniMax International
    apiType: anthropic
    baseUrl: https://api.minimax.io/anthropic
    apiKeyEnvVar: MINIMAX_API_KEY
    tech_spec_url: https://platform.minimax.io/docs/api-reference/text-anthropic-api
    api_mgmt_url: https://platform.minimax.io/
    model_param_options:
      general:
        max_tokens:
          type: integer
          min: 1
          description: Fully supported. Maximum number of tokens to generate.
      anthropic:
        temperature:
          type: number
          min: 0
          max: 1
          default: 1
          description: 'Fully supported. Range (0.0, 1.0], controls output randomness; recommended value: 1.'
        top_p:
          type: number
          min: 0
          max: 1
          description: Fully supported. Nucleus sampling parameter.
        json_response:
          type: boolean
          description: Force JSON object output via Anthropic tool_choice mode.
    models:
      MiniMax-M2.5:
        name: MiniMax M2.5
        context_length: 204800
        input_length: 204800
        output_length: 8192
        context_window: '204K'
      MiniMax-M2.1:
        name: MiniMax M2.1
        context_length: 204800
        input_length: 204800
        output_length: 8192
        context_window: '204K'
      MiniMax-M2:
        name: MiniMax M2 Stable
        context_length: 204800
        input_length: 204800
        output_length: 8192
        context_window: '204K'
  bigmodel:
    name: BigModel
    apiType: anthropic
    baseUrl: https://open.bigmodel.cn/api/anthropic
    apiKeyEnvVar: ZHIPUAI_API_KEY
    tech_spec_url: https://docs.bigmodel.cn/
    api_mgmt_url: https://open.bigmodel.cn/usercenter/apikeys
    models:
      glm-4.7:
        name: GLM-4.7
        context_length: 200000
        input_length: 200000
        output_length: 8192
        context_window: '200K'
      glm-4.6:
        name: GLM-4.6
        context_length: 200000
        input_length: 200000
        output_length: 8192
        context_window: '200K'
      glm-4.5:
        name: GLM-4.5
        context_length: 128000
        input_length: 128000
        output_length: 8192
        context_window: '128K'
      glm-4.5-air:
        name: GLM-4.5-Air
        context_length: 128000
        input_length: 128000
        output_length: 8192
        context_window: '128K'
  z.ai:
    name: Z.AI
    apiType: anthropic
    baseUrl: https://api.z.ai/api/anthropic
    apiKeyEnvVar: ZAI_API_KEY
    tech_spec_url: https://docs.z.ai/api-reference
    api_mgmt_url: https://z.ai/manage-apikey/apikey-list
    models:
      glm-4.7:
        name: GLM-4.7
        context_length: 200000
        input_length: 200000
        output_length: 8192
        context_window: '200K'
      glm-4.6:
        name: GLM-4.6
        context_length: 200000
        input_length: 200000
        output_length: 8192
        context_window: '200K'
      glm-4.5:
        name: GLM-4.5
        context_length: 128000
        input_length: 128000
        output_length: 8192
        context_window: '128K'
      glm-4.5-air:
        name: GLM-4.5-Air
        context_length: 128000
        input_length: 128000
        output_length: 8192
        context_window: '128K'
  volcano-engine-coding-plan:
    name: Volcano Engine Coding Plan
    apiType: anthropic
    baseUrl: https://ark.cn-beijing.volces.com/api/coding
    apiKeyEnvVar: ARK_API_KEY
    tech_spec_url: https://api.volcengine.com/api-docs/view?serviceCode=ark&version=2024-01-01
    api_mgmt_url: https://console.volcengine.com/ark
    models:
      doubao-seed-code-preview-latest:
        name: Doubao Seed Code Preview
        context_length: 256000
        input_length: 256000
        output_length: 8192
        context_window: '256K'
        optimization: 专为Agentic Coding任务优化
  volcano-engine:
    name: Volcano Engine
    apiType: openai-compatible
    baseUrl: https://ark.cn-beijing.volces.com/api/v3
    apiKeyEnvVar: ARK_API_KEY
    tech_spec_url: https://api.volcengine.com/api-docs/view?serviceCode=ark&version=2024-01-01
    api_mgmt_url: https://console.volcengine.com/ark
    models:
      deepseek-v3-2-251201:
        name: DeepSeek-V3.2
        context_length: 128000
        input_length: 96000
        output_length: 32000
        context_window: '128K'
      doubao-seed-code-preview-251028:
        name: Doubao Seed Code Preview 251028
        context_length: 256000
        input_length: 256000
        output_length: 8192
        context_window: '256K'
      doubao-seed-1-6-251015:
        name: Doubao Seed 1.6
        context_length: 256000
        input_length: 256000
        output_length: 8192
        context_window: '256K'
      doubao-seed-1-6-thinking-250715:
        name: Doubao Seed 1.6 Thinking
        context_length: 256000
        input_length: 256000
        output_length: 8192
        context_window: '256K'
      doubao-seed-1-6-lite-251015:
        name: Doubao Seed 1.6 Lite
        context_length: 32000
        input_length: 32000
        output_length: 8192
        context_window: '32K'
      kimi-k2-250905:
        name: Kimi K2
        context_length: 200000
        input_length: 200000
        output_length: 8192
        context_window: '200K (约20万汉字)'
      deepseek-v3-1-terminus:
        name: DeepSeek V3.1 Terminus
        context_length: 131072
        input_length: 131072
        output_length: 8192
        context_window: '128K'
  anthropic:
    name: Anthropic
    apiType: anthropic
    baseUrl: https://api.anthropic.com
    apiKeyEnvVar: ANTHROPIC_API_KEY
    tech_spec_url: https://docs.anthropic.com/en/docs
    api_mgmt_url: https://console.anthropic.com/
    model_param_options:
      general:
        max_tokens:
          type: integer
          min: 1
          description: Maximum tokens to generate (provider-agnostic override).
      anthropic:
        temperature:
          type: number
          min: 0
          max: 1
          description: Sampling temperature (0–1).
        top_p:
          type: number
          min: 0
          max: 1
          description: Nucleus sampling probability (0–1).
        json_response:
          type: boolean
          description: Force JSON object output via Anthropic tool_choice mode.
        top_k:
          type: integer
          min: 0
          description: Top-k sampling (0 disables).
        max_tokens:
          type: integer
          min: 1
          description: Provider-specific max tokens override.
        stop_sequences:
          type: string_array
          description: Stop sequences (array of strings).
        reasoning_split:
          # If supported, streams a separated "thinking/reasoning" channel.
          type: boolean
          description: Enable separated reasoning stream (when supported).
    models:
      claude-opus-4.5:
        name: Claude Opus 4.5
        context_length: 200000
        input_length: 200000
        output_length: 8192
        context_window: '200K'
      claude-sonnet-4.5:
        name: Claude Sonnet 4.5
        context_length: 200000
        input_length: 200000
        output_length: 8192
        context_window: '200K'
      claude-haiku-4.5:
        name: Claude Haiku 4.5
        context_length: 200000
        input_length: 200000
        output_length: 8192
        context_window: '200K'
  openai:
    name: OpenAI
    apiType: openai
    baseUrl: https://api.openai.com/v1
    apiKeyEnvVar: OPENAI_API_KEY
    tech_spec_url: https://platform.openai.com/docs
    api_mgmt_url: https://platform.openai.com/api-keys
    model_param_options:
      general:
        max_tokens:
          type: integer
          min: 1
          description: Maximum tokens to generate (provider-agnostic override).
      openai:
        reasoning_effort:
          type: enum
          values: [none, minimal, low, medium, high, xhigh]
          description: Reasoning effort level (when supported by the model).
        verbosity:
          type: enum
          values: [low, medium, high]
          description: Response verbosity/detail level (GPT-5 series).
        temperature:
          type: number
          min: 0
          max: 2
          description: Sampling temperature (0–2).
        top_p:
          type: number
          min: 0
          max: 1
          description: Nucleus sampling probability (0–1).
        json_response:
          type: boolean
          description: Force JSON object output mode.
        max_tokens:
          type: integer
          min: 1
          description: Provider-specific max tokens override.
    models:
      gpt-5.2:
        name: GPT-5.2
        context_length: 272000
        input_length: 272000
        output_length: 32768
        context_window: '272K'
      gpt-5.2-codex:
        name: GPT-5.2 Codex
        context_length: 272000
        input_length: 272000
        output_length: 32768
        context_window: '272K'
